Using pip 25.1.1 from /home/sam/anaconda3/envs/rag/lib/python3.13/site-packages/pip (python 3.13)
Collecting llama-cpp-python
  Downloading llama_cpp_python-0.3.14.tar.gz (51.0 MB)
     ━━━━━━━━━━━━━━━ 51.0/51.0 MB 60.0 MB/s eta 0:00:00
  Installing build dependencies: started
  Running command pip subprocess to install build dependencies
  Using pip 25.1.1 from /home/sam/anaconda3/envs/rag/lib/python3.13/site-packages/pip (python 3.13)
  Collecting scikit-build-core>=0.9.2 (from scikit-build-core[pyproject]>=0.9.2)
    Obtaining dependency information for scikit-build-core>=0.9.2 from https://files.pythonhosted.org/packages/45/23/0ffa0df7550ca0535f6e03b9a9ab2bf0495ac62e15fd322544c98321a10c/scikit_build_core-0.11.5-py3-none-any.whl.metadata
    Using cached scikit_build_core-0.11.5-py3-none-any.whl.metadata (18 kB)
  Collecting packaging>=23.2 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)
    Obtaining dependency information for packaging>=23.2 from https://files.pythonhosted.org/packages/20/12/38679034af332785aac8774540895e234f4d07f7545804097de4b666afd8/packaging-25.0-py3-none-any.whl.metadata
    Using cached packaging-25.0-py3-none-any.whl.metadata (3.3 kB)
  Collecting pathspec>=0.10.1 (from scikit-build-core>=0.9.2->scikit-build-core[pyproject]>=0.9.2)
    Obtaining dependency information for pathspec>=0.10.1 from https://files.pythonhosted.org/packages/cc/20/ff623b09d963f88bfde16306a54e12ee5ea43e9b597108672ff3a408aad6/pathspec-0.12.1-py3-none-any.whl.metadata
    Using cached pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)
  Using cached scikit_build_core-0.11.5-py3-none-any.whl (185 kB)
  Using cached packaging-25.0-py3-none-any.whl (66 kB)
  Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)
  Installing collected packages: pathspec, packaging, scikit-build-core

  Successfully installed packaging-25.0 pathspec-0.12.1 scikit-build-core-0.11.5
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Running command Getting requirements to build wheel
  Getting requirements to build wheel: finished with status 'done'
  Installing backend dependencies: started
  Running command pip subprocess to install backend dependencies
  Using pip 25.1.1 from /home/sam/anaconda3/envs/rag/lib/python3.13/site-packages/pip (python 3.13)
  Collecting ninja>=1.5
    Obtaining dependency information for ninja>=1.5 from https://files.pythonhosted.org/packages/eb/7a/455d2877fe6cf99886849c7f9755d897df32eaf3a0fba47b56e615f880f7/ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata
    Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)
  Using cached ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)
  Installing collected packages: ninja
  Successfully installed ninja-1.11.1.4
  Installing backend dependencies: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Running command Preparing metadata (pyproject.toml)
  *** scikit-build-core 0.11.5 using CMake 3.28.3 (metadata_wheel)
  Preparing metadata (pyproject.toml): finished with status 'done'
Collecting typing-extensions>=4.5.0 (from llama-cpp-python)
  Obtaining dependency information for typing-extensions>=4.5.0 from https://files.pythonhosted.org/packages/b5/00/d631e67a838026495268c2f6884f3711a15a9a2a96cd244fdaea53b823fb/typing_extensions-4.14.1-py3-none-any.whl.metadata
  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)
  Link requires a different Python (3.13.5 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/3a/be/650f9c091ef71cb01d735775d554e068752d3ff63d7943b26316dc401749/numpy-1.21.2.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)
  Link requires a different Python (3.13.5 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/5f/d6/ad58ded26556eaeaa8c971e08b6466f17c4ac4d786cd3d800e26ce59cc01/numpy-1.21.3.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)
  Link requires a different Python (3.13.5 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/fb/48/b0708ebd7718a8933f0d3937513ef8ef2f4f04529f1f66ca86d873043921/numpy-1.21.4.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)
  Link requires a different Python (3.13.5 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/c2/a8/a924a09492bdfee8c2ec3094d0a13f2799800b4fdc9c890738aeeb12c72e/numpy-1.21.5.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)
  Link requires a different Python (3.13.5 not in: '>=3.7,<3.11'): https://files.pythonhosted.org/packages/45/b7/de7b8e67f2232c26af57c205aaad29fe17754f793404f59c8a730c7a191a/numpy-1.21.6.zip (from https://pypi.org/simple/numpy/) (requires-python:>=3.7,<3.11)
  Link requires a different Python (3.13.5 not in: '<3.13,>=3.9'): https://files.pythonhosted.org/packages/55/b3/b13bce39ba82b7398c06d10446f5ffd5c07db39b09bd37370dc720c7951c/numpy-1.26.0.tar.gz (from https://pypi.org/simple/numpy/) (requires-python:<3.13,>=3.9)
  Link requires a different Python (3.13.5 not in: '<3.13,>=3.9'): https://files.pythonhosted.org/packages/78/23/f78fd8311e0f710fe1d065d50b92ce0057fe877b8ed7fd41b28ad6865bfc/numpy-1.26.1.tar.gz (from https://pypi.org/simple/numpy/) (requires-python:<3.13,>=3.9)
Collecting numpy>=1.20.0 (from llama-cpp-python)
  Obtaining dependency information for numpy>=1.20.0 from https://files.pythonhosted.org/packages/50/30/af1b277b443f2fb08acf1c55ce9d68ee540043f158630d62cef012750f9f/numpy-2.3.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata
  Downloading numpy-2.3.1-cp313-cp313-manylinux_2_28_x86_64.whl.metadata (62 kB)
Collecting diskcache>=5.6.1 (from llama-cpp-python)
  Obtaining dependency information for diskcache>=5.6.1 from https://files.pythonhosted.org/packages/3f/27/4570e78fc0bf5ea0ca45eb1de3818a23787af9b390c0b0a0033a1b8236f9/diskcache-5.6.3-py3-none-any.whl.metadata
  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)
Collecting jinja2>=2.11.3 (from llama-cpp-python)
  Obtaining dependency information for jinja2>=2.11.3 from https://files.pythonhosted.org/packages/62/a1/3d680cbfd5f4b8f15abc1d571870c5fc3e594bb582bc3b64ea099db13e56/jinja2-3.1.6-py3-none-any.whl.metadata
  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)
  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/0c/91/96cf928db8236f1bfab6ce15ad070dfdd02ed88261c2afafd4b43575e9e9/MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata
  Downloading MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)
Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)
Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)
Downloading MarkupSafe-3.0.2-cp313-cp313-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)
Downloading numpy-2.3.1-cp313-cp313-manylinux_2_28_x86_64.whl (16.6 MB)
   ━━━━━━━━━━━━━━━━━ 16.6/16.6 MB 63.9 MB/s eta 0:00:00
Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)
Building wheels for collected packages: llama-cpp-python
  Building wheel for llama-cpp-python (pyproject.toml): started
  Running command Building wheel for llama-cpp-python (pyproject.toml)
  *** scikit-build-core 0.11.5 using CMake 3.28.3 (wheel)
  *** Configuring CMake...
  loading initial cache file /tmp/tmpkd12uolc/build/CMakeInit.txt
  -- The C compiler identification is GNU 12.4.0
  -- The CXX compiler identification is GNU 12.4.0
  -- Detecting C compiler ABI info
  -- Detecting C compiler ABI info - done
  -- Check for working C compiler: /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-cc - skipped
  -- Detecting C compile features
  -- Detecting C compile features - done
  -- Detecting CXX compiler ABI info
  -- Detecting CXX compiler ABI info - done
  -- Check for working CXX compiler: /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ - skipped
  -- Detecting CXX compile features
  -- Detecting CXX compile features - done
  -- Found Git: /usr/bin/git (found version "2.43.0")
  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD
  -- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Failed
  -- Check if compiler accepts -pthread
  -- Check if compiler accepts -pthread - yes
  -- Found Threads: TRUE
  -- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF
  -- CMAKE_SYSTEM_PROCESSOR: x86_64
  -- GGML_SYSTEM_ARCH: x86
  -- Including CPU backend
  -- Found OpenMP_C: -fopenmp (found version "4.5")
  -- Found OpenMP_CXX: -fopenmp (found version "4.5")
  -- Found OpenMP: TRUE (found version "4.5")
  -- x86 detected
  -- Adding CPU backend variant ggml-cpu: -march=native
  -- Found CUDAToolkit: /usr/local/cuda-12.9/targets/x86_64-linux/include (found version "12.9.86")
  -- CUDA Toolkit found
  -- Using CUDA architectures: native
  -- The CUDA compiler identification is NVIDIA 12.9.86
  -- Detecting CUDA compiler ABI info
  -- Detecting CUDA compiler ABI info - done
  -- Check for working CUDA compiler: /usr/local/cuda-12.9/bin/nvcc - skipped
  -- Detecting CUDA compile features
  -- Detecting CUDA compile features - done
  -- CUDA host compiler is GNU 13.3.0
  -- Including CUDA backend
  -- ggml version: 0.0.1
  -- ggml commit:  79e0b68
  -- Looking for pthread_create in pthreads
  -- Looking for pthread_create in pthreads - not found
  -- Looking for pthread_create in pthread
  -- Looking for pthread_create in pthread - found
  CMake Warning (dev) at CMakeLists.txt:13 (install):
    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  Call Stack (most recent call first):
    CMakeLists.txt:108 (llama_cpp_python_install_target)
  This warning is for project developers.  Use -Wno-dev to suppress it.

  CMake Warning (dev) at CMakeLists.txt:21 (install):
    Target llama has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  Call Stack (most recent call first):
    CMakeLists.txt:108 (llama_cpp_python_install_target)
  This warning is for project developers.  Use -Wno-dev to suppress it.

  CMake Warning (dev) at CMakeLists.txt:13 (install):
    Target ggml has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  Call Stack (most recent call first):
    CMakeLists.txt:109 (llama_cpp_python_install_target)
  This warning is for project developers.  Use -Wno-dev to suppress it.

  CMake Warning (dev) at CMakeLists.txt:21 (install):
    Target ggml has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  Call Stack (most recent call first):
    CMakeLists.txt:109 (llama_cpp_python_install_target)
  This warning is for project developers.  Use -Wno-dev to suppress it.

  CMake Warning (dev) at CMakeLists.txt:13 (install):
    Target mtmd has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  Call Stack (most recent call first):
    CMakeLists.txt:162 (llama_cpp_python_install_target)
  This warning is for project developers.  Use -Wno-dev to suppress it.

  CMake Warning (dev) at CMakeLists.txt:21 (install):
    Target mtmd has PUBLIC_HEADER files but no PUBLIC_HEADER DESTINATION.
  Call Stack (most recent call first):
    CMakeLists.txt:162 (llama_cpp_python_install_target)
  This warning is for project developers.  Use -Wno-dev to suppress it.

  -- Configuring done (7.3s)
  -- Generating done (0.0s)
  -- Build files have been written to: /tmp/tmpkd12uolc/build
  *** Building project with Ninja...
  Change Dir: '/tmp/tmpkd12uolc/build'

  Run Build Command(s): ninja -v
  [1/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/hbm.cpp
  [2/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BUILD -DGGML_COMMIT=\"79e0b68\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -pthread -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml.cpp
  [3/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BUILD -DGGML_COMMIT=\"79e0b68\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -pthread -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-threading.cpp
  [4/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-cc -DGGML_BUILD -DGGML_COMMIT=\"79e0b68\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -pthread -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-alloc.c
  [5/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BUILD -DGGML_COMMIT=\"79e0b68\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -pthread -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-backend.cpp
  [6/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/traits.cpp
  [7/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-cc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.c
  [8/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/amx/amx.cpp
  [9/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/amx/mmq.cpp
  [10/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/vec.cpp
  [11/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_BUILD -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-backend-reg.cpp
  [12/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BUILD -DGGML_COMMIT=\"79e0b68\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -pthread -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-opt.cpp
  [13/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/arch/x86/repack.cpp
  [14/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-cc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/arch/x86/quants.c
  [15/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/ggml-cpu.cpp
  [16/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-cc -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/quants.c
  [17/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/repack.cpp
  In function 'block_q4_0x4 make_block_q4_0x4(block_q4_0*, unsigned int)',
      inlined from 'int repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)' at /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/repack.cpp:940:39:
  /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/repack.cpp:793:19: warning: writing 32 bytes into a region of size 0 [-Wstringop-overflow=]
    793 |             memcpy(&out.qs[dst_offset], &elems, sizeof(uint64_t));
        |             ~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/repack.cpp: In function 'int repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)':
  /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/repack.cpp:940:20: note: at offset 72 into destination object '<anonymous>' of size 72
    940 |             *dst++ = make_block_q4_0x4(dst_tmp, interleave_block);
        |             ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  In function 'block_q4_0x4 make_block_q4_0x4(block_q4_0*, unsigned int)',
      inlined from 'int repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)' at /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/repack.cpp:940:39:
  /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/repack.cpp:793:19: warning: writing 32 bytes into a region of size 0 [-Wstringop-overflow=]
    793 |             memcpy(&out.qs[dst_offset], &elems, sizeof(uint64_t));
        |             ~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/repack.cpp: In function 'int repack_q4_0_to_q4_0_4_bl(ggml_tensor*, int, const void*, size_t)':
  /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/repack.cpp:940:20: note: at offset 104 into destination object '<anonymous>' of size 72
    940 |             *dst++ = make_block_q4_0x4(dst_tmp, interleave_block);
        |             ~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  [18/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-cc -DGGML_BUILD -DGGML_COMMIT=\"79e0b68\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -pthread -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml.c
  [19/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/binary-ops.cpp
  [20/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/unary-ops.cpp
  [21/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/argsort.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o
  [22/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/arange.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o
  [23/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/argmax.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o
  [24/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/acc.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o
  [25/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/clamp.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o
  [26/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/concat.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o
  [27/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/diagmask.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o
  [28/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/count-equal.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o
  [29/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/conv-transpose-1d.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o
  [30/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/llamafile/sgemm.cpp
  [31/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/conv2d-dw.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o
  [32/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BUILD -DGGML_COMMIT=\"79e0b68\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -pthread -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/gguf.cpp
  [33/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/conv2d-transpose.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o
  [34/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/cross-entropy-loss.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o
  [35/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/fattn.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o
  [36/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f16.cu.o
  [37/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/fattn-tile-f32.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile-f32.cu.o
  [38/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/cpy.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o
  [39/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-cc -DGGML_BUILD -DGGML_COMMIT=\"79e0b68\" -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_VERSION=\"0.0.1\" -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_base_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu11 -fPIC -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wdouble-promotion -pthread -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-quants.c
  [40/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/binbcast.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o
  [41/177] : && /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -fPIC -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG  -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/sam/anaconda3/envs/rag/lib -Wl,-rpath-link,/home/sam/anaconda3/envs/rag/lib -L/home/sam/anaconda3/envs/rag/lib -shared -Wl,-soname,libggml-base.so -o bin/libggml-base.so vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o  -Wl,-rpath,"\$ORIGIN"  -lm  -pthread && :
  [42/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/mean.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o
  [43/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -DGGML_USE_CPU_REPACK -DGGML_USE_LLAMAFILE -DGGML_USE_OPENMP -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cpu_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -std=gnu++17 -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -march=native -fopenmp -MD -MT vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o -MF vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o.d -o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cpu/ops.cpp
  [44/177] : && /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -fPIC -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG  -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/sam/anaconda3/envs/rag/lib -Wl,-rpath-link,/home/sam/anaconda3/envs/rag/lib -L/home/sam/anaconda3/envs/rag/lib -shared -Wl,-soname,libggml-cpu.so -o bin/libggml-cpu.so vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o vendor/llama.cpp/ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o  -Wl,-rpath,"\$ORIGIN"  bin/libggml-base.so  /home/sam/anaconda3/envs/rag/lib/libgomp.so  -lpthread && :
  [45/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/convert.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o
  [46/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/mmq.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o
  [47/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/gla.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o
  [48/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/im2col.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o
  [49/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/getrows.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o
  [50/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/out-prod.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o
  [51/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/pad.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o
  [52/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/opt-step-adamw.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o
  [53/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/pool2d.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o
  [54/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/quantize.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o
  [55/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/scale.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o
  [56/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/set-rows.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o
  [57/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/norm.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o
  [58/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/ssm-conv.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o
  [59/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/sumrows.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o
  [60/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/rope.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o
  [61/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/tsembd.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o
  [62/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/ssm-scan.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o
  [63/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o
  /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu: In function ‘bool ggml_backend_cuda_device_supports_op(ggml_backend_dev_t, const ggml_tensor*)’:
  /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/ggml-cuda.cu:3229:133: note: ‘#pragma message: TODO: implement Q4_0, Q4_1, Q5_0, Q5_1, Q8_0, IQ4_NL support (https://github.com/ggml-org/llama.cpp/pull/14661)’
   3229 | #pragma message("TODO: implement Q4_0, Q4_1, Q5_0, Q5_1, Q8_0, IQ4_NL support (https://github.com/ggml-org/llama.cpp/pull/14661)")
        |                                                                                                                                     ^
  [64/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/upscale.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o
  [65/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/fattn-wmma-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o
  [66/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/softmax.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o
  [67/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/unary.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o
  [68/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/wkv.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o
  [69/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o
  [70/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o
  [71/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o
  [72/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/sum.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o
  [73/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o
  [74/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o
  [75/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o
  [76/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o
  [77/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o
  [78/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o
  [79/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o
  [80/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o
  [81/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o
  [82/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o
  [83/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o
  [84/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o
  [85/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o
  [86/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o
  [87/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o
  [88/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o
  [89/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmv.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmv.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/mmv.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmv.cu.o
  [90/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/mmvq.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o
  [91/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q4_0-q4_0.cu.o
  [92/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q4_0-q4_0.cu.o
  [93/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-q8_0-q8_0.cu.o
  [94/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama.cpp
  [95/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs128-f16-f16.cu.o
  [96/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-q8_0-q8_0.cu.o
  [97/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs256-f16-f16.cu.o
  [98/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-adapter.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-adapter.cpp
  [99/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f16-instance-hs64-f16-f16.cu.o
  [100/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-cparams.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-cparams.cpp
  [101/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-arch.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-arch.cpp
  [102/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs128-f16-f16.cu.o
  [103/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-batch.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-batch.cpp
  [104/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-hparams.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-hparams.cpp
  [105/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-chat.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-chat.cpp
  [106/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-io.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-io.cpp
  [107/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-impl.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-impl.cpp
  [108/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs64-f16-f16.cu.o
  [109/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-context.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-context.cpp
  [110/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-memory.cpp
  [111/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified-iswa.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-kv-cache-unified-iswa.cpp
  [112/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-vec-f32-instance-hs256-f16-f16.cu.o
  [113/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-graph.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-graph.cpp
  [114/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-mmap.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-mmap.cpp
  [115/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-recurrent.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-memory-recurrent.cpp
  [116/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-saver.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-model-saver.cpp
  [117/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-memory-hybrid.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-memory-hybrid.cpp
  [118/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-kv-cache-unified.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-kv-cache-unified.cpp
  [119/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/unicode-data.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/unicode-data.cpp
  [120/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++   -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/build_info.dir/build-info.cpp.o -c /tmp/tmpkd12uolc/build/vendor/llama.cpp/common/build-info.cpp
  [121/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-model-loader.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-model-loader.cpp
  [122/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-grammar.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-grammar.cpp
  [123/177] /usr/local/cuda-12.9/bin/nvcc -forward-unknown-to-host-compiler -DGGML_BACKEND_BUILD -DGGML_BACKEND_SHARED -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 -DGGML_CUDA_USE_GRAPHS -DGGML_SCHED_MAX_COPIES=4 -DGGML_SHARED -D_GNU_SOURCE -D_XOPEN_SOURCE=600 -Dggml_cuda_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/.. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -isystem /usr/local/cuda-12.9/targets/x86_64-linux/include -O3 -DNDEBUG -std=c++17 -arch=native -Xcompiler=-fPIC -use_fast_math -extended-lambda -compress-mode=size -Xcompiler "-Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -Wno-pedantic" -MD -MT vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o -MF vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o.d -x cu -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/ggml-cuda/template-instances/mmq-instance-iq2_xxs.cu -o vendor/llama.cpp/ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o
  [124/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-quant.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-quant.cpp
  [125/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_SHARED -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/common/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/common/../vendor -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -pthread -MD -MT vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -MF vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o.d -o vendor/llama.cpp/common/CMakeFiles/common.dir/console.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/common/console.cpp
  [126/177] /home/sam/anaconda3/envs/rag/bin/x86_64-conda-linux-gnu-c++ -DGGML_BACKEND_SHARED -DGGML_SHARED -DGGML_USE_CPU -DGGML_USE_CUDA -DLLAMA_BUILD -DLLAMA_SHARED -Dllama_EXPORTS -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/. -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/../include -I/tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/ggml/src/../include -fvisibility-inlines-hidden -fmessage-length=0 -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/sam/anaconda3/envs/rag/include -O3 -DNDEBUG -fPIC -Wmissing-declarations -Wmissing-noreturn -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-array-bounds -Wextra-semi -MD -MT vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o -MF vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o.d -o vendor/llama.cpp/src/CMakeFiles/llama.dir/llama-vocab.cpp.o -c /tmp/pip-install-9ww6wllq/llama-cpp-python_8656f4ac36eb4de68ec237078c4ab872/vendor/llama.cpp/src/llama-vocab.cpp
  Building wheel for llama-cpp-python (pyproject.toml): finished with status 'canceled'
ERROR: Operation cancelled by user
